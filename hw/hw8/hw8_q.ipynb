{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33c5b0c-d0f1-40c0-b794-3b3471ac73d2",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 8: Word embeddings, time series, and communication\n",
    "### Associated lectures: Lectures 16, 18, 19, and ML communication \n",
    "\n",
    "**Due date: April 08, 2022 at 11:59pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851ce52-f981-4c4e-9e9e-a740edd12e41",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Submission instructions](#sg) (4%)\n",
    "- [Exercise 1 - Exploring pre-trained word embeddings](#1) (24%)\n",
    "- [Exercise 2 - Exploring time series data](#2) (16%)\n",
    "- [Exercise 3 - Short answer questions](#4) (10%)\n",
    "- [Exercise 4 - Communication](#4) (46%)\n",
    "- (Optional)[Exercise 5 - Course take away](#5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086914c2-5de1-414a-8770-23bef9f312d0",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe22cb5-f825-4dba-b5e3-3538f4afe703",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "<hr>\n",
    "rubric={points:4}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "**You may work on this homework in a group and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 16, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings [work well on a variety of text classification tasks](http://www.lrec-conf.org/proceedings/lrec2018/pdf/721.pdf). These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads pre-trained word vectors trained on Wikipedia. (The vectors are created using an algorithm called GloVe.) To run the code, you'll need `gensim` package for that in your cpsc330 conda environment, which you can install as follows. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in these pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {},
   "source": [
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points:4}\n",
    "\n",
    "Now that we have GloVe Wiki vectors (`glove_wiki_vectors`) loaded, let's explore the word vectors. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of the model.\n",
    "2. Do the similarities make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31c946-ccba-4432-9687-ebbbadf293cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aadd1aa6-6bb8-48d7-a959-691e19d411ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ca2a9-eba8-472e-a49b-9d0561d96846",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f530299-4ea9-42d7-821e-beedf7404b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7000272\n",
      "0.546276\n",
      "0.6432488\n",
      "0.7552732\n",
      "0.8798075\n",
      "0.076719455\n"
     ]
    }
   ],
   "source": [
    "for w1,w2 in word_pairs: print(glove_wiki_vectors.similarity(w1,w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0331ffe-bf58-4198-bd1e-3b62a5d34319",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d528120-c1ff-4203-82b8-404b62ba6bb0",
   "metadata": {},
   "source": [
    "### 1.2 Bias in embeddings\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "1. In Lecture 16 we saw that our pre-trained word embedding model output an analogy that reinforced a gender stereotype. Give an example of how using such a model could cause harm in the real world.\n",
    "2. Here we are using pre-trained embeddings which are built using Wikipedia data. Explore whether there are any worrisome biases present in these embeddings or not by trying out some examples. You can use the following two methods or other methods of your choice to explore what kind of stereotypes and biases are encoded in these embeddings. \n",
    "    - You can use the `analogy` function below which gives words analogies. \n",
    "    - You can also use [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods. (An example is shown below.)   \n",
    "3. Discuss your observations. Do you observe the gender stereotype we observed in class in these embeddings?\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f7ef2b-d6b4-4338-a153-77fd4fc9847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d900c0-3027-4b9f-a750-bca946cf7bdb",
   "metadata": {},
   "source": [
    "An example of using similarity between words to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f0bacc-ba70-43e3-a7be-07c263f48048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.447236"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e6671-c6db-4cc9-9652-faba038e8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_wiki_vectors.similarity(\"black\", \"rich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf431d-a56d-4d78-b381-eee21bceb049",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ccd3e-25c1-413d-b079-043fb2949090",
   "metadata": {},
   "source": [
    "### 1.3 Representation of all words in English\n",
    "rubric={reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. Do you think it contains **all** words in English language? What would happen if you try to get a word vector that's unlikely to be present in the vocabulary (e.g., the word \"cpsc330\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589eeae-c081-4042-8cdc-7dbc85a2c400",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "464a0fbf-3a9c-42b3-b9cc-5dc474bc2804",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3cd04-30c9-43f4-9b07-6443ab4ecd7d",
   "metadata": {},
   "source": [
    "### 1.4 Classification with pre-trained embeddings\n",
    "rubric={points:8}\n",
    "\n",
    "In lecture 16, we saw that you can conveniently get word vectors with `spaCy` with `en_core_web_md` model. In this exercise, you'll use word embeddings in multi-class text classification task. We will use [HappyDB](https://www.kaggle.com/ritresearch/happydb) corpus which contains about 100,000 happy moments classified into 7 categories: *affection, exercise, bonding, nature, leisure, achievement, enjoy_the_moment*. The data was crowd-sourced via [Amazon Mechanical Turk](https://www.mturk.com/). The ground truth label is not available for all examples, and in this lab, we'll only use the examples where ground truth is available (~15,000 examples). \n",
    "\n",
    "- Download the data from [here](https://www.kaggle.com/ritresearch/happydb).\n",
    "- Unzip the file and copy it in the lab directory.\n",
    "\n",
    "We will be using spaCy in this exercise. If you do not have spaCy in your course environment, here is how you can install it.  \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c conda-forge spacy\n",
    "```\n",
    "\n",
    "- You also need to download the language model which contains all the pre-trained models. For that run the following in your course `conda` environment. \n",
    "```\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "The code below reads the data CSV (assuming that it's present in the current directory as *cleaned_hm.csv*),  cleans it up a bit, and splits it into train and test splits. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Train logistic regression with bag-of-words features and show classification report on the test set. \n",
    "2. Train logistic regression with average embedding representation extracted using spaCy and show classification report on the test set. (You can find an example of extracting average embedding features using spaCy in [lecture 16](https://ubc-cs.github.io/cpsc330/lectures/16_natural-language-processing.html#sentiment-classification-using-average-embeddings).)  \n",
    "3. Discuss your results. Which model is performing well. Which model would be more interpretable?  \n",
    "4. Are you observing any benefits of transfer learning here? Briefly discuss. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b35845-7976-4cda-b798-0c0700868fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cpsc330/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>original_hm</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>modified</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>ground_truth_category</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>206</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.</td>\n",
       "      <td>We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>45</td>\n",
       "      <td>24h</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>leisure</td>\n",
       "      <td>leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27697</th>\n",
       "      <td>498</td>\n",
       "      <td>24h</td>\n",
       "      <td>My grandmother start to walk from the bed after a long time.</td>\n",
       "      <td>My grandmother start to walk from the bed after a long time.</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>affection</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27705</th>\n",
       "      <td>5732</td>\n",
       "      <td>24h</td>\n",
       "      <td>I picked my daughter up from the airport and we have a fun and good conversation on the way home.</td>\n",
       "      <td>I picked my daughter up from the airport and we have a fun and good conversation on the way home.</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bonding</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27715</th>\n",
       "      <td>2272</td>\n",
       "      <td>24h</td>\n",
       "      <td>when i received flowers from my best friend</td>\n",
       "      <td>when i received flowers from my best friend</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        wid reflection_period  \\\n",
       "hmid                            \n",
       "27676  206   24h                \n",
       "27678  45    24h                \n",
       "27697  498   24h                \n",
       "27705  5732  24h                \n",
       "27715  2272  24h                \n",
       "\n",
       "                                                                                                                              original_hm  \\\n",
       "hmid                                                                                                                                        \n",
       "27676  We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.   \n",
       "27678  I meditated last night.                                                                                                              \n",
       "27697  My grandmother start to walk from the bed after a long time.                                                                         \n",
       "27705  I picked my daughter up from the airport and we have a fun and good conversation on the way home.                                    \n",
       "27715  when i received flowers from my best friend                                                                                          \n",
       "\n",
       "                                                                                                                               cleaned_hm  \\\n",
       "hmid                                                                                                                                        \n",
       "27676  We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.   \n",
       "27678  I meditated last night.                                                                                                              \n",
       "27697  My grandmother start to walk from the bed after a long time.                                                                         \n",
       "27705  I picked my daughter up from the airport and we have a fun and good conversation on the way home.                                    \n",
       "27715  when i received flowers from my best friend                                                                                          \n",
       "\n",
       "      modified  num_sentence ground_truth_category predicted_category  \n",
       "hmid                                                                   \n",
       "27676  True     2.0           bonding               bonding            \n",
       "27678  True     1.0           leisure               leisure            \n",
       "27697  True     1.0           affection             affection          \n",
       "27705  True     1.0           bonding               affection          \n",
       "27715  True     1.0           bonding               bonding            "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_hm.csv\", index_col=0)\n",
    "sample_df = df.dropna()\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "384fa13b-83a5-4e23-9280-c4e529937143",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXKa7qfQXYPD",
    "outputId": "8bbf5eeb-0151-4853-a49c-3876279bbeb7"
   },
   "outputs": [],
   "source": [
    "sample_df = sample_df.rename(\n",
    "    columns={\"cleaned_hm\": \"moment\", \"ground_truth_category\": \"target\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de04d594-7174-409d-a9fa-c2fd738e2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(sample_df, test_size=0.3, random_state=123)\n",
    "X_train, y_train = train_df[\"moment\"], train_df[\"target\"]\n",
    "X_test, y_test = test_df[\"moment\"], test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a04e47-984f-4172-b773-c415d87f81fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8736,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f63a73-0d13-4276-9e79-7ad60575a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d3b9cc0-4ed1-42a8-8849-a59aa2522245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix shape: (8736, 7578)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ts/qwpb6qpj6h33x19ldq_x_c5r0000gn/T/ipykernel_2247/1079065444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m pipe_aer = make_pipeline(\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "    CountVectorizer(stop_words=\"english\"), LogisticRegression(max_iter=1000)\n",
    ")\n",
    "pipe.named_steps[\"countvectorizer\"].fit(X_train)\n",
    "X_train_transformed = pipe.named_steps[\"countvectorizer\"].transform(X_train)\n",
    "print(\"Data matrix shape:\", X_train_transformed.shape)\n",
    "pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72945f6a-2fdd-4b15-8293-68e49060a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_train)])\n",
    "X_test_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daf1000e-efb0-4694-8b7e-e70dd4dd56ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001619</td>\n",
       "      <td>0.163005</td>\n",
       "      <td>-0.127113</td>\n",
       "      <td>0.029059</td>\n",
       "      <td>0.105410</td>\n",
       "      <td>-0.073965</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>-0.114335</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>2.426061</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135363</td>\n",
       "      <td>0.027811</td>\n",
       "      <td>0.087899</td>\n",
       "      <td>-0.032443</td>\n",
       "      <td>0.121402</td>\n",
       "      <td>-0.026162</td>\n",
       "      <td>0.015158</td>\n",
       "      <td>0.018964</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.045326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138092</td>\n",
       "      <td>0.111099</td>\n",
       "      <td>-0.176662</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.213636</td>\n",
       "      <td>-0.074071</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>-0.308452</td>\n",
       "      <td>0.011545</td>\n",
       "      <td>2.307457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>0.018923</td>\n",
       "      <td>0.105070</td>\n",
       "      <td>-0.042593</td>\n",
       "      <td>0.200726</td>\n",
       "      <td>-0.040234</td>\n",
       "      <td>0.128420</td>\n",
       "      <td>-0.004693</td>\n",
       "      <td>-0.043989</td>\n",
       "      <td>0.159791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.071637</td>\n",
       "      <td>0.243982</td>\n",
       "      <td>-0.220019</td>\n",
       "      <td>-0.120554</td>\n",
       "      <td>0.131543</td>\n",
       "      <td>0.058194</td>\n",
       "      <td>0.033223</td>\n",
       "      <td>-0.277834</td>\n",
       "      <td>-0.038054</td>\n",
       "      <td>2.568843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013466</td>\n",
       "      <td>-0.048918</td>\n",
       "      <td>0.057614</td>\n",
       "      <td>0.089386</td>\n",
       "      <td>0.100302</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.095061</td>\n",
       "      <td>0.115481</td>\n",
       "      <td>-0.041366</td>\n",
       "      <td>0.034526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270038</td>\n",
       "      <td>0.046558</td>\n",
       "      <td>-0.203693</td>\n",
       "      <td>-0.159842</td>\n",
       "      <td>0.220544</td>\n",
       "      <td>0.126133</td>\n",
       "      <td>0.102225</td>\n",
       "      <td>-0.318787</td>\n",
       "      <td>0.234527</td>\n",
       "      <td>2.232625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025125</td>\n",
       "      <td>0.213690</td>\n",
       "      <td>0.039392</td>\n",
       "      <td>0.109369</td>\n",
       "      <td>0.185680</td>\n",
       "      <td>-0.152662</td>\n",
       "      <td>-0.033732</td>\n",
       "      <td>0.090231</td>\n",
       "      <td>0.069840</td>\n",
       "      <td>0.241330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.062695</td>\n",
       "      <td>0.200277</td>\n",
       "      <td>-0.261488</td>\n",
       "      <td>-0.120265</td>\n",
       "      <td>-0.055726</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.028335</td>\n",
       "      <td>-0.265238</td>\n",
       "      <td>0.017448</td>\n",
       "      <td>2.289553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210378</td>\n",
       "      <td>0.061941</td>\n",
       "      <td>0.039948</td>\n",
       "      <td>-0.013864</td>\n",
       "      <td>0.150718</td>\n",
       "      <td>-0.142158</td>\n",
       "      <td>0.012099</td>\n",
       "      <td>0.052855</td>\n",
       "      <td>0.110244</td>\n",
       "      <td>0.164680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>0.052164</td>\n",
       "      <td>0.212554</td>\n",
       "      <td>-0.266573</td>\n",
       "      <td>-0.010500</td>\n",
       "      <td>0.197077</td>\n",
       "      <td>0.013809</td>\n",
       "      <td>0.092261</td>\n",
       "      <td>-0.298128</td>\n",
       "      <td>-0.022332</td>\n",
       "      <td>2.334018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041334</td>\n",
       "      <td>-0.053246</td>\n",
       "      <td>0.129640</td>\n",
       "      <td>0.259176</td>\n",
       "      <td>0.200732</td>\n",
       "      <td>0.150284</td>\n",
       "      <td>0.121929</td>\n",
       "      <td>0.059759</td>\n",
       "      <td>0.013387</td>\n",
       "      <td>0.137447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8732</th>\n",
       "      <td>-0.000509</td>\n",
       "      <td>0.245367</td>\n",
       "      <td>-0.133633</td>\n",
       "      <td>-0.147443</td>\n",
       "      <td>0.042747</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.095391</td>\n",
       "      <td>-0.227271</td>\n",
       "      <td>0.072037</td>\n",
       "      <td>2.227267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072149</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.018894</td>\n",
       "      <td>0.019263</td>\n",
       "      <td>0.223151</td>\n",
       "      <td>-0.053588</td>\n",
       "      <td>-0.003886</td>\n",
       "      <td>0.009138</td>\n",
       "      <td>-0.087546</td>\n",
       "      <td>0.151940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8733</th>\n",
       "      <td>0.021802</td>\n",
       "      <td>0.163314</td>\n",
       "      <td>-0.157471</td>\n",
       "      <td>-0.236304</td>\n",
       "      <td>-0.059398</td>\n",
       "      <td>0.087349</td>\n",
       "      <td>0.048762</td>\n",
       "      <td>-0.089285</td>\n",
       "      <td>-0.002209</td>\n",
       "      <td>2.237956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178441</td>\n",
       "      <td>0.157714</td>\n",
       "      <td>-0.205210</td>\n",
       "      <td>-0.015405</td>\n",
       "      <td>0.066741</td>\n",
       "      <td>-0.100855</td>\n",
       "      <td>0.031090</td>\n",
       "      <td>-0.164334</td>\n",
       "      <td>0.130502</td>\n",
       "      <td>0.180473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8734</th>\n",
       "      <td>0.274405</td>\n",
       "      <td>0.264138</td>\n",
       "      <td>-0.209930</td>\n",
       "      <td>-0.332012</td>\n",
       "      <td>0.112581</td>\n",
       "      <td>0.029644</td>\n",
       "      <td>0.100134</td>\n",
       "      <td>-0.341042</td>\n",
       "      <td>0.023078</td>\n",
       "      <td>2.198900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191712</td>\n",
       "      <td>0.016046</td>\n",
       "      <td>-0.052890</td>\n",
       "      <td>0.036108</td>\n",
       "      <td>0.257981</td>\n",
       "      <td>-0.146323</td>\n",
       "      <td>-0.059790</td>\n",
       "      <td>0.199103</td>\n",
       "      <td>0.042947</td>\n",
       "      <td>0.016847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8735</th>\n",
       "      <td>-0.019543</td>\n",
       "      <td>0.156579</td>\n",
       "      <td>-0.220960</td>\n",
       "      <td>-0.074639</td>\n",
       "      <td>0.164081</td>\n",
       "      <td>0.060956</td>\n",
       "      <td>0.131320</td>\n",
       "      <td>-0.221994</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>2.040816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070358</td>\n",
       "      <td>0.061059</td>\n",
       "      <td>-0.015886</td>\n",
       "      <td>-0.110327</td>\n",
       "      <td>0.029095</td>\n",
       "      <td>0.046518</td>\n",
       "      <td>-0.034726</td>\n",
       "      <td>-0.038621</td>\n",
       "      <td>0.015602</td>\n",
       "      <td>0.229949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8736 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.001619  0.163005 -0.127113  0.029059  0.105410 -0.073965  0.022733   \n",
       "1     0.138092  0.111099 -0.176662  0.006394  0.213636 -0.074071  0.002759   \n",
       "2    -0.071637  0.243982 -0.220019 -0.120554  0.131543  0.058194  0.033223   \n",
       "3     0.270038  0.046558 -0.203693 -0.159842  0.220544  0.126133  0.102225   \n",
       "4    -0.062695  0.200277 -0.261488 -0.120265 -0.055726  0.018198  0.028335   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8731  0.052164  0.212554 -0.266573 -0.010500  0.197077  0.013809  0.092261   \n",
       "8732 -0.000509  0.245367 -0.133633 -0.147443  0.042747  0.018598  0.095391   \n",
       "8733  0.021802  0.163314 -0.157471 -0.236304 -0.059398  0.087349  0.048762   \n",
       "8734  0.274405  0.264138 -0.209930 -0.332012  0.112581  0.029644  0.100134   \n",
       "8735 -0.019543  0.156579 -0.220960 -0.074639  0.164081  0.060956  0.131320   \n",
       "\n",
       "             7         8         9  ...       290       291       292  \\\n",
       "0    -0.114335  0.018400  2.426061  ... -0.135363  0.027811  0.087899   \n",
       "1    -0.308452  0.011545  2.307457  ... -0.025123  0.018923  0.105070   \n",
       "2    -0.277834 -0.038054  2.568843  ... -0.013466 -0.048918  0.057614   \n",
       "3    -0.318787  0.234527  2.232625  ... -0.025125  0.213690  0.039392   \n",
       "4    -0.265238  0.017448  2.289553  ... -0.210378  0.061941  0.039948   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8731 -0.298128 -0.022332  2.334018  ... -0.041334 -0.053246  0.129640   \n",
       "8732 -0.227271  0.072037  2.227267  ... -0.072149  0.004788  0.018894   \n",
       "8733 -0.089285 -0.002209  2.237956  ... -0.178441  0.157714 -0.205210   \n",
       "8734 -0.341042  0.023078  2.198900  ...  0.191712  0.016046 -0.052890   \n",
       "8735 -0.221994  0.034014  2.040816  ... -0.070358  0.061059 -0.015886   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0    -0.032443  0.121402 -0.026162  0.015158  0.018964  0.002068  0.045326  \n",
       "1    -0.042593  0.200726 -0.040234  0.128420 -0.004693 -0.043989  0.159791  \n",
       "2     0.089386  0.100302  0.011655  0.095061  0.115481 -0.041366  0.034526  \n",
       "3     0.109369  0.185680 -0.152662 -0.033732  0.090231  0.069840  0.241330  \n",
       "4    -0.013864  0.150718 -0.142158  0.012099  0.052855  0.110244  0.164680  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8731  0.259176  0.200732  0.150284  0.121929  0.059759  0.013387  0.137447  \n",
       "8732  0.019263  0.223151 -0.053588 -0.003886  0.009138 -0.087546  0.151940  \n",
       "8733 -0.015405  0.066741 -0.100855  0.031090 -0.164334  0.130502  0.180473  \n",
       "8734  0.036108  0.257981 -0.146323 -0.059790  0.199103  0.042947  0.016847  \n",
       "8735 -0.110327  0.029095  0.046518 -0.034726 -0.038621  0.015602  0.229949  \n",
       "\n",
       "[8736 rows x 300 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec620e19-016a-4476-bb7e-de0c402078d2",
   "metadata": {},
   "source": [
    "## Exercise 2: Exploring time series data <a name=\"2\"></a>\n",
    "<hr>\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b22471fe-942e-49aa-8fb8-9d4fbf6b00b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0 2015-12-27  1.33          64236.62      1036.74  54454.85   48.16    \n",
       "1 2015-12-20  1.35          54876.98      674.28   44638.81   58.33    \n",
       "2 2015-12-13  0.93          118220.22     794.70   109149.67  130.50   \n",
       "3 2015-12-06  1.08          78992.15      1132.00  71976.41   72.58    \n",
       "4 2015-11-29  1.28          51039.60      941.48   43838.39   75.78    \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0  8696.87     8603.62     93.25       0.0          conventional  2015  Albany  \n",
       "1  9505.56     9408.07     97.49       0.0          conventional  2015  Albany  \n",
       "2  8145.35     8042.21     103.14      0.0          conventional  2015  Albany  \n",
       "3  5811.16     5677.40     133.76      0.0          conventional  2015  Albany  \n",
       "4  6183.95     5986.26     197.69      0.0          conventional  2015  Albany  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3425e59d-9580-4512-8bd2-c8c9b836df8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d0aa8d9-34b6-4401-8b91-77e1342510ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-04 00:00:00')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b64f1d1-9614-44df-b625-52a77c00af9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-25 00:00:00')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae5238-ac94-4b1b-b368-d972b6582d8a",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~3 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d4d6fb5-1cbb-47e0-a5c4-34b66d026d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = \"20170925\"\n",
    "train_df = df[df[\"Date\"] <= split_date]\n",
    "test_df = df[df[\"Date\"] > split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26a3b6ae-1406-48c3-8b5c-e7b65a041852",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_df) + len(test_df) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ed401-224b-42d3-a4bd-f67b60c3625b",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain is Australia dataset from lecture, we had different measurements for each Location. What about this dataset: for which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d3fbd-8c98-4e3f-8ca5-22571c563d1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac17a187-bf66-4339-b5f4-3f79cb6948cc",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b56b13-e2ff-45d9-b800-fb7006f92653",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f48e7c-1394-42d7-b481-e910c9ad8986",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16dc1348-c2c6-46f6-bbdf-a77810930ac1",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab120c-6b2b-4dfb-8765-7367a9577482",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain is Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c557cb1f-155f-4eb2-99fe-b93473ba15fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "331ec42a-3093-46c5-b708-ca7716f939dc",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb32828-c8f6-4344-90bc-7ec214e448e7",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We would like to forecast the avocado price, which is the `AveragePrice` column. The function below is adapted from Lecture 18, with some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ef9e53-9170-4a0c-a249-7cf0715496b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_feature(\n",
    "    df, orig_feature, lag, groupby, new_feature_name=None, clip=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a new feature that's a lagged version of an existing one.\n",
    "\n",
    "    NOTE: assumes df is already sorted by the time columns and has unique indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        The column name of the feature we're copying\n",
    "    lag : int\n",
    "        The lag; negative lag means values from the past, positive lag means values from the future\n",
    "    groupby : list\n",
    "        Column(s) to group by in case df contains multiple time series\n",
    "    new_feature_name : str\n",
    "        Override the default name of the newly created column\n",
    "    clip : bool\n",
    "        If True, remove rows with a NaN values for the new feature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "\n",
    "    TODO: could/should simplify this function by using `df.shift()`\n",
    "    \"\"\"\n",
    "\n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = \"%s_lag%d\" % (orig_feature, -lag)\n",
    "        else:\n",
    "            new_feature_name = \"%s_ahead%d\" % (orig_feature, lag)\n",
    "\n",
    "    new_df = df.assign(**{new_feature_name: np.nan})\n",
    "    for name, group in new_df.groupby(groupby):\n",
    "        if lag < 0:  # take values from the past\n",
    "            new_df.loc[group.index[-lag:], new_feature_name] = group.iloc[:lag][\n",
    "                orig_feature\n",
    "            ].values\n",
    "        else:  # take values from the future\n",
    "            new_df.loc[group.index[:-lag], new_feature_name] = group.iloc[lag:][\n",
    "                orig_feature\n",
    "            ].values\n",
    "\n",
    "    if clip:\n",
    "        new_df = new_df.dropna(subset=[new_feature_name])\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbe62a-05a6-4bed-8ae9-b1bb7299c16a",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0049d46-d7e2-40a4-9d41-74ce2e875a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04  1.22          40873.28      2819.50  28287.42  49.90    \n",
       "1     2015-01-11  1.24          41195.08      1002.85  31640.34  127.12   \n",
       "2     2015-01-18  1.17          44511.28      914.14   31540.32  135.77   \n",
       "3     2015-01-25  1.06          45147.50      941.38   33196.16  164.14   \n",
       "4     2015-02-01  0.99          70873.60      1353.90  60017.20  179.32   \n",
       "...          ...   ...               ...          ...       ...     ...   \n",
       "18244 2018-02-25  1.57          18421.24      1974.26  2482.65   0.00     \n",
       "18245 2018-03-04  1.54          17393.30      1832.24  1905.57   0.00     \n",
       "18246 2018-03-11  1.56          22128.42      2162.67  3194.25   8.93     \n",
       "18247 2018-03-18  1.56          15896.38      2055.35  1499.55   0.00     \n",
       "18248 2018-03-25  1.62          15303.40      2325.30  2171.66   0.00     \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0      9716.46     9186.93     529.53      0.0          conventional  2015   \n",
       "1      8424.77     8036.04     388.73      0.0          conventional  2015   \n",
       "2      11921.05    11651.09    269.96      0.0          conventional  2015   \n",
       "3      10845.82    10103.35    742.47      0.0          conventional  2015   \n",
       "4      9323.18     9170.82     152.36      0.0          conventional  2015   \n",
       "...        ...         ...        ...      ...                   ...   ...   \n",
       "18244  13964.33    13698.27    266.06      0.0          organic       2018   \n",
       "18245  13655.49    13401.93    253.56      0.0          organic       2018   \n",
       "18246  16762.57    16510.32    252.25      0.0          organic       2018   \n",
       "18247  12341.48    12114.81    226.67      0.0          organic       2018   \n",
       "18248  10806.44    10569.80    236.64      0.0          organic       2018   \n",
       "\n",
       "                 region  \n",
       "0      Albany            \n",
       "1      Albany            \n",
       "2      Albany            \n",
       "3      Albany            \n",
       "4      Albany            \n",
       "...       ...            \n",
       "18244  WestTexNewMexico  \n",
       "18245  WestTexNewMexico  \n",
       "18246  WestTexNewMexico  \n",
       "18247  WestTexNewMexico  \n",
       "18248  WestTexNewMexico  \n",
       "\n",
       "[18249 rows x 13 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaee71e-d45c-48dc-81a3-ebf7195cbea4",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90736df7-04b7-40d4-a835-e8eb899da7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "      <th>AveragePriceNextWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18243</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>17597.12</td>\n",
       "      <td>1892.05</td>\n",
       "      <td>1928.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13776.71</td>\n",
       "      <td>13553.53</td>\n",
       "      <td>223.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18141 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04  1.22          40873.28      2819.50  28287.42  49.90    \n",
       "1     2015-01-11  1.24          41195.08      1002.85  31640.34  127.12   \n",
       "2     2015-01-18  1.17          44511.28      914.14   31540.32  135.77   \n",
       "3     2015-01-25  1.06          45147.50      941.38   33196.16  164.14   \n",
       "4     2015-02-01  0.99          70873.60      1353.90  60017.20  179.32   \n",
       "...          ...   ...               ...          ...       ...     ...   \n",
       "18243 2018-02-18  1.56          17597.12      1892.05  1928.36   0.00     \n",
       "18244 2018-02-25  1.57          18421.24      1974.26  2482.65   0.00     \n",
       "18245 2018-03-04  1.54          17393.30      1832.24  1905.57   0.00     \n",
       "18246 2018-03-11  1.56          22128.42      2162.67  3194.25   8.93     \n",
       "18247 2018-03-18  1.56          15896.38      2055.35  1499.55   0.00     \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0      9716.46     9186.93     529.53      0.0          conventional  2015   \n",
       "1      8424.77     8036.04     388.73      0.0          conventional  2015   \n",
       "2      11921.05    11651.09    269.96      0.0          conventional  2015   \n",
       "3      10845.82    10103.35    742.47      0.0          conventional  2015   \n",
       "4      9323.18     9170.82     152.36      0.0          conventional  2015   \n",
       "...        ...         ...        ...      ...                   ...   ...   \n",
       "18243  13776.71    13553.53    223.18      0.0          organic       2018   \n",
       "18244  13964.33    13698.27    266.06      0.0          organic       2018   \n",
       "18245  13655.49    13401.93    253.56      0.0          organic       2018   \n",
       "18246  16762.57    16510.32    252.25      0.0          organic       2018   \n",
       "18247  12341.48    12114.81    226.67      0.0          organic       2018   \n",
       "\n",
       "                 region  AveragePriceNextWeek  \n",
       "0      Albany            1.24                  \n",
       "1      Albany            1.17                  \n",
       "2      Albany            1.06                  \n",
       "3      Albany            0.99                  \n",
       "4      Albany            0.99                  \n",
       "...       ...             ...                  \n",
       "18243  WestTexNewMexico  1.57                  \n",
       "18244  WestTexNewMexico  1.54                  \n",
       "18245  WestTexNewMexico  1.56                  \n",
       "18246  WestTexNewMexico  1.56                  \n",
       "18247  WestTexNewMexico  1.62                  \n",
       "\n",
       "[18141 rows x 14 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hastarget = create_lag_feature(\n",
    "    df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True\n",
    ")\n",
    "df_hastarget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfa73e-48d7-49be-af1c-dba397d9f946",
   "metadata": {},
   "source": [
    "I will now split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e952769-dbd1-4052-855f-2d2f2b4101d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "test_df = df_hastarget[df_hastarget[\"Date\"] > split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb19b2-0a17-4133-bf78-3dea9aa61526",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e951f-8dde-4a34-bf4e-b2c3a6270bb0",
   "metadata": {},
   "source": [
    "### 2.4 Baseline\n",
    "rubric={points:4}\n",
    "\n",
    "Let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8369f38-926b-4428-a276-dc1719b94d50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0d241cd-4669-46d9-bb21-1fd8006a39e8",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0615dc-63c2-458f-844b-f17d30758ab1",
   "metadata": {},
   "source": [
    "### (Optional) 2.5 Modeling\n",
    "rubric={points:2}\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "> Because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe2285-38d1-409a-9f4c-3f5dfd50622a",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad9e61-e5f6-4025-8356-f48f651a8e1e",
   "metadata": {},
   "source": [
    "## Exercise 3: Short answer questions <a name=\"3\"></a>\n",
    "\n",
    "Each question is worth 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972060d9-742d-47ae-82c5-74b258de16e7",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "rubric={points:4}\n",
    "\n",
    "The following questions pertain to Lecture 18 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb63fb-80d3-4cd4-a1f0-38ed8bfe3a21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef053d93-f20e-417f-81ae-35edb701020c",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3703ae-7d56-4987-b70b-4e222f1d8b15",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 19 on survival analysis. We'll consider the use case of customer churn analysis.\n",
    "\n",
    "1. What is the problem with simply labeling customers are \"churned\" or \"not churned\" and using standard supervised learning techniques, as we did in hw5?\n",
    "2. Consider customer A who just joined last week vs. customer B who has been with the service for a year. Who do you expect will leave the service first: probably customer A, probably customer B, or we don't have enough information to answer?\n",
    "3. If a customer's survival function is almost flat during a certain period, how do we interpret that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8507d-3e72-469b-a946-05c816be18a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8ffdd0-8c1e-4042-8414-ac8e57caf980",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594c68-91c3-45d9-baec-6ac38a02c971",
   "metadata": {},
   "source": [
    "## Exercise 4: Communication <a name=\"4\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767926d-c17d-4a93-b59a-7242a4c76ff0",
   "metadata": {},
   "source": [
    "### 4.1 Blog post \n",
    "rubric={points:40}\n",
    "\n",
    "Write up your analysis from hw6 or any other assignment or your side project on machine learning in a \"blog post\" or report format. It's fine if you just write it here in this notebook. Alternatively, you can publish your blog post publicly and include a link here. (See exercise 4.3.) The target audience for your blog post is someone like yourself right before you took this course. They don't necessarily have ML knowledge, but they have a solid foundation in technical matters. The post should focus on explaining **your results and what you did** in a way that's understandable to such a person, **not** a lesson trying to teach someone about machine learning. Again: focus on the results and why they are interesting; avoid pedagogical content.\n",
    "\n",
    "Your post must include the following elements (not necessarily in this order):\n",
    "\n",
    "- Description of the problem/decision.\n",
    "- Description of the dataset (the raw data and/or some EDA).\n",
    "- Description of the model.\n",
    "- Description your results, both quantitatively and qualitatively. Make sure to refer to the original problem/decision.\n",
    "- A section on caveats, describing at least 3 reasons why your results might be incorrect, misleading, overconfident, or otherwise problematic. Make reference to your specific dataset, model, approach, etc. To check that your reasons are specific enough, make sure they would not make sense, if left unchanged, to most students' submissions; for example, do not just say \"overfitting\" without explaining why you might be worried about overfitting in your specific case.\n",
    "- At least 3 visualizations. These visualizations must be embedded/interwoven into the text, not pasted at the end. The text must refer directly to each visualization. For example \"as shown below\" or \"the figure demonstrates\" or \"take a look at Figure 1\", etc. It is **not** sufficient to put a visualization in without referring to it directly.\n",
    "\n",
    "A reasonable length for your entire post would be **800 words**. The maximum allowed is **1000 words**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169eefb-18a8-4e13-b7ca-1b3539a79215",
   "metadata": {},
   "source": [
    "#### Example blog posts\n",
    "\n",
    "Here are some examples of applied ML blog posts that you may find useful as inspiration. The target audiences of these posts aren't necessarily the same as yours, and these posts are longer than yours, but they are well-structured and engaging. You are **not required to read these** posts as part of this assignment - they are here only as examples if you'd find that useful.\n",
    "\n",
    "From the UBC Master of Data Science blog, written by a past student:\n",
    "\n",
    "- https://ubc-mds.github.io/2019-07-26-predicting-customer-probabilities/\n",
    "\n",
    "This next one uses R instead of Python, but that might be good in a way, as you can see what it's like for a reader that doesn't understand the code itself (the target audience for your post here):\n",
    "\n",
    "- https://rpubs.com/RosieB/taylorswiftlyricanalysis\n",
    "\n",
    "Finally, here are a couple interviews with winners from Kaggle competitions. The format isn't quite the same as a blog post, but you might find them interesting/relevant:\n",
    "\n",
    "- https://medium.com/kaggle-blog/instacart-market-basket-analysis-feda2700cded\n",
    "- https://medium.com/kaggle-blog/winner-interview-with-shivam-bansal-data-science-for-good-challenge-city-of-los-angeles-3294c0ed1fb2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfdd094-eeb6-4f00-a3bc-5a6105eedb12",
   "metadata": {},
   "source": [
    "#### A note on plagiarism\n",
    "\n",
    "You may **NOT** include text or visualizations that were not written/created by you. If you are in any doubt as to what constitutes plagiarism, please just ask. For more information see the [UBC Academic Misconduct policies](http://www.calendar.ubc.ca/vancouver/index.cfm?tree=3,54,111,959). Please don't copy this from somewhere ðŸ™. If you can't do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052395d-a695-4063-97b6-46c4e13016d8",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59667c9-db6a-4c12-a556-5b9815ef3564",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "rubric={points:6}\n",
    "\n",
    "Describe one effective communication technique that you used in your post, or an aspect of the post that you are particularly satisfied with.\n",
    "\n",
    "Max 3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea9c37-34c9-4b3e-a2df-e00cedd3e8ae",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56965b8c-9f4d-4a68-be4c-2a745dcf9989",
   "metadata": {},
   "source": [
    "## (optional, not for marks) 4.3\n",
    "\n",
    "Publish your blog post from 4.1 publicly using a tool like Hugo, or somewhere like medium.com, and paste a link here. Be sure to pick a tool in which code and code output look reasonable. This link could be a useful line on your resume!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a55e63-38cb-4e36-867f-0261d1c583de",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cefc8e-cf76-4c27-9aa6-c560cbc0fc2b",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 5 <a name=\"5\"></a>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from this course? \n",
    "\n",
    "> I'm looking forward to read your answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb9e2f-a2d2-4f56-9fb4-c91492e4801b",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab723dc5-4ea6-4c44-ace9-bf345bf8c120",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from â€œ1â€ will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e160c-d947-4123-8e67-fa3c89c9aa8f",
   "metadata": {},
   "source": [
    "### Congratulations on finishing all homework assignments! :clap: :clap: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3bee4-0171-4465-838f-e5ac8a943e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"eva-congrats.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
