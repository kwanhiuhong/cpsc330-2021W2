11.1 Ensembles
Model that combines multiple ML models to create a more powerful model

11.2 Gradient boosted tree
unlike random forest where each tree is independent, GBT has no ramdomization and builds trees in a serial manner, where each tree tries to correct the mistakes of the previous one. (GBT combines many simple models/shallow tree with depth 1-5)

hyperparam of GBT:
1.n_estimators: the higher the more complex
2.learning_rate: controls how strongly each tree tries to correct the mistakes of the previous trees. Higher means more complex as well 

In general for tree based models, you don't need to scale features (that's usually used when you are using linear models)


class lecture
random_forest

"predict" of random forest is done by voting(classification) or average(regression) of predictions given by individual tree/model

n_estimators decide how many decision trees we want to build

Gradient boosted trees





