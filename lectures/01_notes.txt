2.1 ML Terminology
1. features = regressors = independent variables
2. dependent variable = target
3. fit = train

2.2 Baseline Model
Baselines
(A simple ML algo based on simple rules - say picking/always predicting most frequent labels E.g. 11 ppl not getting A+, 10 ppl getting A+, if we always predict not A+, then 52% correct rate)

If any of your ML algo is worse than the baseline, then your ML model is garbage.

Steps to train a classifier using sklearn:
1.read the data
2.create x and y
3.create a classifier object
4.fit the classisier
5.predict on new samples
6.score the model

A. DummyClassifier for Baseline Classification Model
"from sklearn.dummy import DummyClassifier"
"dummy_clf = DummyClassifier(strategy="most_frequent")"
this is the classifier for the baseline model in sklearn

The score() in classification problem = number of correct predictions/samples size

B. DummyRegressor for Baseline Regression Model
"from sklearn.dummy import DummyRegressor"
"dummy_reg = DummyRegressor()"
this is the dummy regressor for the baseline model in sklearn

The score() in regression problem is the R^2 score!
R^2 can be NEGATIVE!!! That means your model is predicting really badly

The first step in training a machine learning model is to first train the dummy classifier/regressor/baseline model so that you can compare the score later with your actual trained model!

2.3 Decision Trees (Classification)
!!!We can also use decision tree for regression problems.
https://ubc-cs.github.io/cpsc330/lectures/02_decision-trees.html#decision-tree-for-regression-problems

2.4 More terminology
~ All leaf nodes in decision trees are pure (Only one class of sample in the leaf nodes) The decision tree expansion is done until all leaf nodes are pure and only contain pure leaf nodes

~ Decision stump: A decision tree with only one stump (5:30). Not necessarily all leaf nodes are pure in this case.

Parameters: Things that you learn during trainings
	The decision tree algorithm primarily learns two things:
		1.the best feature to split on
		2.the threshold for the feature to split on at each node
	These are called parameters of the decision tree model. When predicting on new examples, we need parameters of the model.


Hyper-parameters: Things that you set to control the training

Decision Boundary: 

